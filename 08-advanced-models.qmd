# מודלים מונחים (Supervised) {#sec-advanced-models}

## הקדמה

ניתן לחלק את עולם המודלים הסטטיסטיים לשני סוגים, המובחנים ביניהם בדרך שבה בונים אותם:

-   מודלים מונחים (Supervised learning)

-   מודלים לא מונחים (Unsupervised learning)

מודלים מונחים הינם מודלים שנבנים תוך שימוש במשתנה תלוי (כפי שראינו בפרק הקודם של רגרסיה לינארית), ולעומת זאת, מודלים לא מונחים הינם מודלים שבהם אין משתנה תלוי (נניח לדוגמה כשרוצים לחלק את מרחב התצפיות לאשכולות שונים).

בפרק זה נתמקד במידול מונחה, מנקודת מבט מעשית, תוך שימוש בחבילות במשפחה של `tidymodels`.

::: callout-note
## קריאה נוספת לצורך העמקה בתיאוריה

ישנן משפחות רבות של מודלים מונחים, בין המוכרים כלולים גם מודלי שפה (כגון ChatGPT), אך גם מודלים לזיהוי נטישה, זיהוי הונאות, רגרסיות למיניהן, למידה עמוקה, ועוד.

הספר הנוכחי אינו מתמקד בתיאוריה העומדת בבסיס המודלים השונים, אלא מהווה עזר מעשי להפעלת המודלים. אתם מוזמנים לעיין בספרים הבאים לצורך העמקה תיאורתית:

-   The Elements of statistical learning [@hastie2009]

-   An Introduction to Statistical Learning [@james2021]
:::

## תיאור שלבי העבודה

כפי שציינו במבוא לספר זה, תהליך ניתוח הנתונים כולל טרנספורמציה על משתנים, וויז'ואליזציות של המשתנים והקשרים שלהם, ומידול. שלב המידול עצמו כולל מספר תתי-שלבים:

1.  חלוקה אקראית של המדגם לקבוצת אימון (training set), קבוצת ולידציה (validation set) וקבוצת מבחן (test set).
2.  בניית מתכון (recipe) להכנת הדאטה למודל. בניית המתכון משמשת להגדרת כל הטרנספורמציות הדרושות לביצוע על הדאטה, לפני שהוא נכנס למודל.
3.  הגדרת מודל/ים: בשלב זה מגדירים את המודלים בהם הולכים לעשות שימוש. גם מבחינת האלגוריתם של המודל (סוג המודל) וגם מבחינת מטרת המודל (רגרסיה לעומת סיווג).
4.  הגדרת תהליך אימון המודל (workflow) ובניית המודל הראשוני (fitting).
5.  כיוונון המודל (tuning).
6.  הערכת ביצועי המודל (evaluation).
7.  שימוש במודל לחיזוי תצפיות חדשות (prediction).

בשלב 1 אנחנו מפרקים את הנתונים לשלוש קבוצות, כאשר מטרת קבוצת האימון הינה לבנות את המודל, מטרת קבוצת הולידציה הינה לכוון את "היפר-פרמטרים" של המודל (hyper-paramters)ומטרת קבוצת המבחן היא לבחון את שגיאת המודל (על דאטה שהמודל לא תלוי בו משום שלא נלמד על בסיסו). קבוצת האימון תשתמש אותנו בשלבים 2-5, קבוצת הולידציה תשמש אותנו בשלב 6, וקבוצת המבחן תשמש אותנו בשלב 7 בלבד. שלבים 2-6 עשויים לחזור על עצמם מספר פעמים במהלך פיתוח מודלים (עם גרסאות שונות למודל או מודלים שונים לגמרי). השלב האחרון הינו השלב שבו אנחנו מפיקים ערך מהמודל, ומשתמשים בו לחיזוי עבור תצפיות חדשות.

כעת נפרק כל אחד מהשלבים ונסביר עליו בפירוט. על מנת להדגים את השלבים נשתמש בדאטה שמלווה אותנו מתחילת הספר (Palmer penguins), כאשר הבעיה שנפתור בפרק זה היא סיווג של תצפית של פינגויין לזכר או נקבה לפי כל המשתנים האחרים שעומדים לרשותנו.

## חלוקת המדגם

ראשית נחלק את המדגם לשלוש הקבוצות: אימון, ולידציה, ומבחן.

אנחנו קוראים את החבילות הנדרשות, מגדירים Seed, כך שנוכל להגריל חלוקה אקראית אך עקבית (תמיד כשנריץ את הקוד נקבל את אותה החלוקה האקראית), ונפצל את הדאטה לשלושת הקבוצות.

```{r initial splitting of the data}
#| warning: false
#| message: false
library(tidymodels) # reading the meta-package tidymodels
library(palmerpenguins) # reading the palmer penguins data set which include penguins

set.seed(42) # set an initial seed

# Modify the dependent var to a dummy
penguins_new <- penguins %>% 
  mutate(sex = factor(sex))

# Create the split
penguin_split <- initial_validation_split(penguins_new, 
                                          prop = c(0.6, 0.2), 
                                          strata = sex)
penguin_split
```

ראשית לפני התחלה, שינינו את המשתנה sex למשתנה פקטור. שינוי זה יקל עלינו בהמשך בהפעלת מודלים לסיווג. משתנים קטגוריאליים אחרים המהווים משתנים בלתי-תלויים (משתנים מסבירים) יטופלו בהמשך.

כפי שניתן לראות, המדגם חולק לקבוצת אימון בגודל של 205 תצפיות (המהווה כ-60% מהדאטה המקורי), קבוצת ולידציה בגודל 69 תצפיות (המהווה כ-20% מהדאטה המקורי), וקבוצת מבחן בגודל 70 תצפיות (המהווה כ-20%). גודל הקבוצות הוגדר באמצעות שימוש בארגומנט `prop`. מקובל לקבוע את קבוצת האימון כקבוצה הגדולה ביותר. ערכים מקובלים לקבוצת האימון והולידציה הם בדרך כלל 70-80% במצטבר, כאשר גודל קבוצת המבחן יהווה כ-20-30% מהדאטה בהתאמה. עם זאת, אין הגדרות חד-משמעיות בהיבט זה, וניתן לחרוג ממספרים אלו כתלות בצורך.

השתמשנו בארגומנט `strata` על מנת לבצע את הדגימה באופן ששומר על הפרופורציות של מין התצפיות בתוך כל תת-קבוצה. ארגומנט זה שימושי במיוחד כאשר הקבוצות אינן מאוזנות (קבוצה מסוימת קטנה משמעותית מאחרות).

על מנת לחלץ את הדאטאות עצמם של כל אחת מהקבוצות נשתמש בפונקציות הבאות:

```{r extract sets}
penguin_train <- training(penguin_split)
penguin_valid <- validation(penguin_split)
penguin_test <- testing(penguin_split)
```

את המשך הלמידה על הדאטה, כולל בניית המודל, נבצע על קבוצת האימון בלבד `penguin_train`.

::: callout-tip
## תרגיל: הבנת הדאטה לפני צלילה למודל

בדרך כלל לאחר פיצול הדאטה נבצע ויז'ואליזציות שונות כדי להבין איך משתנים שונים משפיעים על המשתנה התלוי (לפני שרואים את השפעתם במודל עצמו). בשלב זה העבודה שתוארה ב[פרק @sec-ggplot2] תהא משמעותית מאוד. לצורך תרגול, בחנו כיצד המשתנים השונים שבדאטה שלנו (penguin_train) משפיעים על מין התצפית.

1.  השתמשו בויז'ואליזציות שונות כפי שנלמדו והודגמו בפרק על ויז'ואליזציות. לאור הבדיקה, אילו משתנים הייתם מצפים שיהיו משמעותיים במודלים שנפתח?
2.  ישנן שלוש תצפיות בעלות ערך חסר (ללא מין) בקבוצת האימון. לא נוכל להשתמש בתצפיות אלו לצורך אימון המודל - הסבירו מדוע.
:::

## בניית מתכון

כפי שמיד תראו, חבילת tidymodels מאמצת ז'רגון של מטבח כמו בניית מתכון - recipe, סחיטת מיצים - juice, אפייה - bake, גזר לבן - parsnip, ועוד.\
בשלב זה נבנה "מתכון" שהמטרה שלו להגדיר את השינויים שעובר הדאטה לפני שהוא נכנס למודל. במובן מסויים, התהליך מאוד דומה לתהליך שתואר ב[פרק @sec-data-munging-tidyverse] על הכנת נתונים, אך יש הבדל מהותי: מתכון זה יופעל אוטומטית בכל פעם שנרצה להפעיל את המודל מחדש על תצפיות חדשות, וכן הוא יופעל גם במהלך שלב הכיוונים (וגם ניתן יהיה להגדיר היפר-פרמטרים כחלק מהמתכון, ולאפשר לשלב הכיוונון לבדוק גם אותם).

בבניית המתכון הבא נפעיל מספר צעדים, החל מהפקודה `recipe` לאתחול המקום ודרך צעדים שונים (`step`) שמיד נסביר אותם.

```{r building a recipe}
# Excuse the pun (for the name...)
penguin_recipe <- recipe(sex ~ ., data = penguin_train) %>% 
  step_naomit(bill_length_mm:sex) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(species, island) %>% 
  step_select(-year) %>% 
  step_corr(bill_length_mm:body_mass_g)

penguin_recipe
```

במתכון שלעיל הגדרנו ארבעה צעדים (פונקציות שמתחילות ב-`step_*`).

בצעד הראשון אנחנו משמיטים ערכים חסרים מהדאטה באמצעות `step_na_omit`. בדאטה שלנו תצפיות בעלות ערך חסר מכילות ערך חסר בכל המשתנים הרלוונטיים, ולכן בחרנו להשמיט אותן לגמרי. באופן כללי, בבעיות אחרות כדאי לנסות לזקוף ערכים חסרים על ידי שימוש בצעדים כגון `step_impute_bag`, `step_impute_knn`, `step_impute_linear`, ועוד לפני שמסירים את התצפיות לגמרי. ביצוע צעד של זקיפת נתונים יאפשר להתמודד עם חיזוי של תצפיות חדשות גם אם הן מגיעות עם ערך חסר, בעוד שצעד הסרה ימנע מאיתנו לספק תחזית.

הצעד הבא, `step_normalize`, הופך את כל המשתנים המספריים (`all_nominal_predictors()`) למשתנים מנורמלים (כלומר בעלי תוחלת 0 וסטיית תקן 1). צעד זה גם ישמור את התוחלת וסטיית התקן המחושבת על קבוצת האימון, וישתמש בהן לנרמול של תצפיות חדשות בשלב החיזוי (חשבו למה).

הצעד הבא, `step_dummy`,הופך משתנים מסוג מחרוזת או פקטור למשתני דמי (בעלי ערכי 0 או 1). במקרה שלנו אלו משתני סוג הפינגויין, האי שבו נמדדה התצפית, ומין הפינגויין (שכבר הומר למשתנה פקטור בתחילת הקוד לפני הפיצול הראשוני לקבוצות). ניתן לשים לב שהמשתנה אי התצפית פוצל לשני משתני דמי (במקור ישנן שלוש רמות Dream, Torgersen, ו-Biscoe, כאשר השניים הראשונים הם 0 המשמעות היא שהתצפית הגיעה מהשלישי), וכך גם המשתנה של סוג הפינגויין (שפוצל לשני משתני דמי Chinstrap, Gentoo, והשלישי Adelie אינו בדאטה ומשתמע מהשניים האחרים).

הצעד `step_select`מוריד את משתנה השנה (שמתעד מתי נמדדה התצפית). הנחת העבודה בהסרת המשתנה היא שהשנה שבה נמדדה התצפית ומין הפינגויינים אינם קשורים סטטיסטית (בהנחה שהתפלגות מין הפינגויינים נשארת קבועה בין השנים). הפעלת הפוקנציה מאוד דומה לפונקציה `select` שבה דנו בפרק הכנת הנתונים.

הצעד האחרון שבחרנו לבצע הינו `step_corr`. צעד זה בוחן קורלציות בין המשתנים ובמידה והוא מזהה משתנה שבעל קורלציות גבוהות למשתנים אחרים, הוא מסיר אותו (במקרה זה לצעד זה אין השפעה, ואף משתנה אינו מוסר משום שאין מולטיקולינאריות, כפי שמיד נראה).

נציין שרשימת הצעדים האפשריים גדולה במיוחד ומבוססת על best practices של הנדסת נתונים (כפי שנקראים בעגה המקצועית - feature engineering). מומלץ להיעזר [ברשימת הצעדים](https://recipes.tidymodels.org/articles/recipes.html) המתעדכנת באתר הרשמי. כמו כן, תמיד ניתן להגדיר צעדים חדשים נוספים, לדוגמה על ידי שימוש ב-`step_mutate` להגדרת משתנה נוסף שהוא תוצאת חישוב של משתנים אחרים, או באופן מתקדם יותר על ידי [פיתוח של פונקציה חדשה](https://www.tidymodels.org/learn/develop/recipes/).

על מנת לבחון את תוצאת המתכון, ניתן להשתמש בפונקציות הכנה ואפייה, באופן הבא:

```{r prep and bake}
penguin_recipe %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  glimpse()
```

באמצעות הפונקציה `prep` אנחנו מנחים את הפעלת הצעדים על בסיס קבוצת האימון, והפונקציה `bake` מריצה אותם בפועל. אולי זה נראה ככפילות מסוימת אבל בעצם יכלנו להריץ את `bake` על כל דאטה (כגון קבוצת הולידציה, קבוצת המבחן, או תצפיות חדשות לגמרי). אם מפעילים את `bake` עם `new_data = NULL` אז היא פשוט תפעיל הכל על קבוצת האימון.

## הגדרת המודל

הבעיה עמה אנחנו מתמודדים בפרק זה היא בעיית סיווג (classification), בהתאם לכך, ישנם מודלי סיווג שבאפשרותנו להשתמש בהם, כגון: רגרסיה לוגיסטית, עצי החלטה, יערות אקראיים, boosting, bagging, ועוד. כפי שציינו ניתן להתעמק במודלים השונים, בשני הספרים שצויינו בתחילת הפרק.\
נדגים הגדרה של שלושה מודלים שונים: רגרסיה לוגיסטית, יער אקראי, ו-boosting.

```{r definition of models}
# Logistic regression
penguin_logistic <- logistic_reg(
  mode = "classification",
  engine = "glm",
)

# Random forest
penguin_forest <- rand_forest(
  mode = "classification",
  engine = "ranger",
  mtry = 3,
  trees = 50,
  min_n = 15)

# Boosting
penguin_boost <- boost_tree(
  mode = "classification",
  engine = "xgboost")
```

שימו לב לכמה נקודות מעניינות:

-   עד כה כל הפעולות שעשינו היו זהות לכל המודלים (פיצול והכנת הדאטה), וזו הפעולה הראשונה שבה אנחנו מבחינים בין שלושה מודלים שונים. שלושת המודלים יקבלו את אותו הדאטה בשלב האימון. העוצמה הגדולה של `tidymodels` היא ביכולת שלה לאפשר לנו ממשק לחבילות מרובות (כפי שמיד נסביר, אנחנו מפעילים פה פונקציות מחבילות `stats`, `ranger`, ו-`xgboost`), תוך שימוש בשפה אחידה. במקור, לכל חבילה שבה אנחנו משתמשים בהגדרה לעיל יש ממשק שונה שצריך ללמוד אותו אם מפעילים ישירות את החבילות, אבל במקרה זה `tidymodels` מספקת לנו מעטפת אחידה.

-   כל מודל מוגדר עם ארגומנט `mode` שמבחין בין בעיות סיווג לבעיות רגרסיה. במקרה זה אנחנו עובדים עם בעיית סיווג, ולכן הארגומנט הוגדר כ-`mode = classification`.

-   בחבילת `parsnip` שהינה חבילה בתוך `tidymodels` ישנם ממשקים שונים להפעלת פונקציות מידול שונות, ובמקרה זה השתמשנו בשלושה ממשקים:

    -   הפונקציה `logistic_reg` שבאמצעות `engine = "glm"` הנחנו אותה לעבוד עם פונקציית `glm` (generalized linear models) שנמצאת בחבילת הבסיס `stats`. מעיון בתיעוד של `logistic_reg` תוכלו לראות שהיא מספקת ממשקים למעל עשר חבילות שונות של רגרסיה לוגיסטית (הכללות שונות של המודל הבסיסי או אלגוריתמים בעלי יעילות טובה יותר).

    -   הפונקציה `rand_forest` גם היא מספקת ממשקים לפוקנציות ואלגוריתמים שונים של יערות אקראיים, ובמקרה זה הנחנו את הפונקציה להוות ממשק לחבילת `ranger`.

    -   הפונקציה `boost_tree` שמייצרת מודלים המורכבים מהרבה תתי מודלים אחרים (ensemble), ובמקרה זה תעבוד כממשק לחבילת `xgboost`.

-   כל פונקציית ממשק כזו תפעיל פונקציות מחבילות אחרות (בדוגמה שלנו מחבילת stats, ranger, ו-xgboost), ולכן עלינו לוודא שהתקנו את החבילות הנדרשות.

-   פונקציות הממשק יודעות להעביר ארגומנטים שונים שרלוונטיים לפונקציות שהן מפעילות. לדוגמה לחבילת `ranger` אנחנו מעבירים את הארגומנט `mtry = 3` שמסביר מכמה משתנים יש לבנות כל עץ שמשתתף ביער שלנו. עיון בתיעוד הפונקציות בחבילות של האלגוריתמים יראה אילו ארגומנטים קיימים.

## הגדרת תהליך ובניית המודל

כעת, על מנת להפעיל את האלגוריתמים השונים לצורך בניית המודלים, נגדיר תהליך בניה ונבנה את המודל בפועל באופן הבא:

```{r define workflow}
penguin_workflow <- workflow() %>% 
  add_recipe(penguin_recipe)

penguin_logistic_fit <- penguin_workflow %>% 
  add_model(penguin_logistic) %>% 
  fit(data = penguin_train)

penguin_forest_fit <- penguin_workflow %>% 
  add_model(penguin_forest) %>% 
  fit(data = penguin_train)

penguin_boost_fit <- penguin_workflow %>% 
  add_model(penguin_boost) %>% 
  fit(data = penguin_train)
```

לפעמים מודל הוא "קופסה שחורה" ולא ניתן להתעמק בו (לדוגמה, מודלי למידה עמוקה, ובפרט מודלי שפה כגון ChatGPT הם קופסה שחורה, בהיבט שקשה לחקור את המודל אך קל לחזות בתוצאותיו).\
עם זאת, לעיתים ההתעמקות בתכונות של מודל יכולה לשפוך אור על הקשר שבין המשתנים (כפי שראינו ב[פרק @sec-linear-regression]. לשם כך עומדות לרשותנו פונקציות שונות המאפשרות לחלץ את המודל וללמוד את תכונותיו.\
לדוגמה, להלן חקירה של מודל הרגרסיה הלוגיסטית:

```{r extract fit and summary of logistic}
penguin_logistic_fit %>% 
  extract_fit_engine() %>% 
  summary()
```

הפלט דומה לפלט הפקודה `summary` כאשר מריצים אותה על מודל הרגרסיה הלינארית שראינו בפרק הקודם אך עם מספר הבדלים, לדוגמה:

-   במקום Residuals אנחנו רואים Deviance Residuals (חישוב שונה של שאריות המבוסס על פונקציית נראות במקום הפרש בין ערך התצפית בפועל לערך החזוי של התצפית).

-   במקום מבחן F וערך של $R^2$ אנחנו רואים ערך של AIC (Akaike Information Criteria) - קריטריון אחר המאפשר להשוות בין מודלים.

-   משמעות המקדמים שונה (Estimate): ברגרסיה לוגיסטית, האקספוננט של המקדם הוא ה-Odds Ratio, כלומר פי כמה שינוי של יחידה במשתנה המסביר מעלה את הסבירות לשינוי הסיווג.

מומלץ להעמיק בתכונות הרגרסיה הלוגיסטית, לדוגמה בספר של @hastie2009.

להלן חקירה של מודל ה-boosting

```{r extract fit and summary of boosting}
penguin_boost_fit %>% 
  extract_fit_engine()

penguin_boost_fit %>% 
  extract_fit_engine()
```

## קריאה נוספת

לצורך קריאה והעמקה נוספת בתהליך המידול, מומלץ לקרוא את [@kuhn2022tidy] המפרט על השימוש בחבילת `tidymodels` עם דוגמאות רבות. הספר [זמין באינטרנט](https://www.tmwr.org/).

## סיכום

::: end-page
