# רגרסיה לינארית {#sec-linear-regression}

בפרק זה נדון באחד המודלים הבסיסיים בסטטיסטיקה: רגרסיה לינארית.

רגרסיה (regression) היא שם כולל למודלים אשר מתאימים בין משתנים בלתי תלויים (המסומנים ב-x בדרך כלל, ונקראים גם משתנים מסבירים), לבין משתנה תלוי (המסומן ב-y בדרך כלל). רגרסיה לינארית הינה סוג מסוים של רגרסיה המניח קשר לינארי בין המשתנה התלוי לבין המשתנים הבלתי תלויים.

כאמור, בפרק זה איננו מתעמקים בתיאוריה הסטטיסטית, ולצורך העמקת הרקע הסטטיסטי שנוגע לבעיות רגרסיה, מומלץ לעיין בספר מתאים [לדוגמה @walpole1993probability]. עם זאת, נציין שמודל הרגרסיה הלינארית מוצא את מקדמי ה-$\beta$ שקושרת בין המשתנה התלוי למשתנים המסבירים: $$
y=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon
$$

המקדמים שמביא מודל הרגרסיה הלינארית מביאים למינימום את **סכום ריבועי השגיאות** אשר נמחיש בהמשך הפרק (בנוסחה לעיל $\epsilon$ מסמן את שגיאת המודל). התחזיות שהמודל מפיק מסומנות ב-$\hat{y}_i$, והממוצע של כלל תצפיות הערך $y$ מסומן ב-$\bar{y}$.

::: callout-note
באופן כללי נציין שיש אבחנה בין בעיות רגרסיה בהן המשתנה התלוי מקבל ערכים רציפים (ערך ממשי), לבין בעיות סיווג (Classification), בהם המשתנה התלוי מקבל ערך קטגוריאלי. נדון באבחנה זו ובדוגמאות נוספות של מודלים בפרק הבא.
:::

```{r setup}
#| include: false
#| 
ggplot2::theme_set(ggplot2::theme_bw())
```

נדגים שימוש בגרסיה לינארית באמצעות הדאטה שעמו עבדנו עד כה, שבו תצפיות של פינגוינים (`palmerpenguins::penguins`). כפי שראינו בפרקים קודמים (ראו לדוגמה ב[פרק @sec-ggplot2] תחת "מיפויים נוספים"), הקשרים שבין המשתנים שבדאטה תלויים גם בסוג הפינגויין, ולכן נתחיל בהסתכלות על הקשר שבין עומק המקור לאורך המקור בזן Adelie בלבד.

התרשים הבא מציג את פיזור התצפיות בין עומק מקור לאורך המקור, וכמו כן מציג רגרסיה לינארית "פשוטה" (כלומר רגרסיה שבה יש משתנה בלתי תלוי יחיד).

המשתנה התלוי בציר x (אורך המקור), והמשתנה הבלתי-תלוי בציר y (עומק המקור). במקרה זה הבחירה במה יהיה המשתנה התלוי ומה המשתנה הבלתי תלוי היא בחירה שרירותית, אז בדרך כלל זהו אינו המצב (ראו הסבר במסגרת שלאחר התרשים).

את קו הרגרסיה אנחנו מוסיפים באמצעות הפקודה `stat_smooth(method="lm")` , אשר מוסיפה לתרשים פיזור החלקה, במקרה זה בשיטה של רגרסיה לינארית (lm זה קיצור של linear model). הרקע האפור מסביב לקו הלינארי של הרגרסיה מהווה רווח בר סמך למודל הרגרסיה (מיד נדון במשמעויות שלו).

```{r adelie plot}
#| warning: false
#| message: false
#| include: true

library(palmerpenguins)
library(tidyverse)

adelie_penguins <- penguins %>% 
  filter(species == "Adelie") %>% 
  filter(!is.na(bill_length_mm), !is.na(bill_depth_mm))

adelie_penguins %>%
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + 
  geom_point() + 
  stat_smooth(method = "lm") + 
  ggtitle("Relationship of bill depth and bill length among Adelie penguins",
          subtitle = "Linear model applied")

```

על פי ההמחשה הויזואלית של קו הרגרסיה, נראה שיש קשר חיובי בין עומק המקור לבין אורך המקור (כאשר עולה האורך, כך עולה גם העומק של המקור). על מנת לקבל את מודל הרגרסיה ניתן להשתמש בפקודה `lm` באופן הבא:

```{r demonstration of lm}
adelie_lm <- lm(formula = bill_depth_mm ~ bill_length_mm,
                data = adelie_penguins)

summary(adelie_lm)
```

השתמשנו בפקודה `summary` שיודעת לקבל אובייקטים שונים (במקרה הזה אובייקט שהוא תוצר של מודל לינארי), ולהציג סיכום שלו הכולל את מקדמי המודל, המובהקויות, וכן מימדים נוספים כגון $r^2$, וסטטיסטי F, שמיד נסביר עליהם.

## פלט הפקודה `lm`

החלק הראשון בפלט (call) מתאר את הפקודה שבה השתמשנו על מנת לקבל את המודל.

### השאריות

החלק השני (residuals) מתאר את שאריות המודל. השאריות הן ההפרש בין הערך האמיתי של y לבין תוצאת המודל (במקרה זה הערך של אורך המקור פחות חיזוי המודל לאורך המקור). בפלט המודל ניתן לראות את המינימום, ואת הרבעונים (ביניהם החציון), ואת המקסימום. ניתן גם לחלץ את השאריות עצמן על ידי הקריאה `adelie_lm$residuals`. באופן כזה ניתן גם להסתכל בהתפלגות עצמה (נבחן זאת בהמשך הפרק).

::: callout-note
שימו לב שבשאריות לא מופיע ממוצע השגיאות. הסיבה לכך היא שבמודל רגרסיה לינארית ממוצע השגיאות הוא תמיד אפס. למעוניינים, ניתן למצוא הוכחות מתמטיות בספרות, וגם במקורות שונים באינטרנט, לדוגמה [בקישור זה](https://math.stackexchange.com/questions/494181/why-the-sum-of-residuals-equals-0-when-we-do-a-sample-regression-by-ols#:~:text=company%20blog-,Why%20the%20sum%20of%20residuals%20equals%200%20when,a%20sample%20regression%20by%20OLS%3F&text=From%20a%20geometric%20point%20of,0%2C%20and%20we%20are%20done.).
:::

אם נקפוץ רגע לתחתית הפלט, נראה שבשורה השלישית מלמטה יש את ה-Residual standard error, שהיא טעות התקן של השאריות. הערך שמופיע בצמוד (149) הוא מספר דרגות החופש של המודל, קרי מספר התצפיות פחות מספר הפרמטרים של המודל, וניתן לחלץ אותו על ידי קריאה ל-`adelie_lm$df.residual`.\
נסו לחשב את `sqrt(sum(adelie_lm$residuals^2)/adelie_lm$df.residual))` על מנת לשחזר את החישוב המופיע בשורה הזו.

### טבלת המקדמים

הטבלה שמתחת לשאריות מציגה את מקדמי הרגרסיה, כאשר ה-Intercept הוא החותך ($\beta_0$) והשורות הבאות מתייחסות למקדמים של המשתנים (במקרה הזה המקדם של המשתנה `bill_length_mm`. העמודה `Estimate` מתייחסת לערך עצמו של המקדם (למעשה מדובר באמד למקדם), העמודה השניה לטעות התקן של המקדם, ושתי העמודות האחרונות מתייחסות למבחן סטטסטי מסוג T-test הבוחן האם המקדם שווה ל-0 (בהשערת האפשר) או שונה מאפס (השערה אלטרנטיבית). העמודה `t value` מתייחסת לערך סטטיסטי T והעמודה `Pr(<|t|)` מתייחסת לערך ה-p-value של המבחן.\
מימין לעמודת ה-p-value ישנו מקרא של ערכי p-value, שמתואר גם מתחת לטבלה, ועוזר לנו לזהות מיידית מקדמים מובהקים (שימושי בטבלאות ארוכות עם הרבה מקדמים).

### שונות מוסברת

אחוז השונות המוסברת, המסומן כ-$R^2$ הוא היחס בין השונות של מודל הרגרסיה (השונות של התחזיות $\hat{y}_i$ שמסומנת כ-$\operatorname{SSR}$) לבין השונות הכללית של המשתנה התלוי ($y_i$ המסומנת ב-$\operatorname{SST}$). הרעיון הוא שככל שהמודל מכיל יותר שונות, כך הוא מצליח להסביר יותר טוב את המשתנה התלוי, לכן נרצה שהשונות המוסברת תהיה גבוהה ככל הניתן (במסגרת מגבלות של Over-fitting שנדון בהם בפרק הבא). השונות המוסברת חסומה בשונות הכללית ($0\leq\operatorname{SSR}\leq\operatorname{SST}$), ולכן $0\leq R^2\leq1$.

בפלט המודל לעיל, ערך השונות המוסברת הינו $R^2=0.1533$. כלומר המשתנה `bill_length_mm` מצליח להסביר כ-15% מהשונות של המשתנה `bill_depth_mm`.

הערך שמסומן בפלט כ-Adjusted R-squared קרוב לערך $R^2$, אך מביא בחשבון את מספר הפרמטרים של המודל (ככל שלמודל יהיו יותר פרמטרים, ערך Adjusted R-squared יהיה נמוך יותר).

### סטטיסטי F

בתחתיתו של פלט המודל מופיע ערכו של סטטיסטי F. סטטיסטי זה מתייחס למבחן הסטטיסטי שבו בהשערת האפס כלל המקדמים הינם אפס($\beta_i=0, \forall i\geq1$), לעומת ההשערה האלטרנטיבית שבה אחד המקדמים שונה מ-0.

::: callout-tip
# תרגיל: ערכי p-value במודל רגרסיה

במודל שלעיל ערך ה-p-value של סטטיסטי F זהה לערך ה-p-value של מקדם `bill_length_mm`. למה?

נסו לבנות מודל רגרסיה עם `bill_length_mm` וגם עם `flipper_length_mm`. האם במודל זה ערכי ה-p-values של המקדמים ושל מבחן F זהים? הסבירו מדוע.
:::

## המחשה של שגיאות המודל

כעת נמחיש את שגיאות המודל. התרשים הבא מציג מדגם של 25 תצפיות, בציר x המשתנה `bill_length_mm` ובציר ה-y המשתנה התלוי `bill_depth_mm`.

```{r illustration of regression residuals}
set.seed(0)
adelie_penguins %>% 
  mutate(model_predictions = predict(adelie_lm)) %>% 
  sample_n(25) %>% 
  ggplot() + 
  geom_line(aes(x = bill_length_mm, y = model_predictions)) + 
  geom_point(aes(x = bill_length_mm, y = bill_depth_mm)) + 
  geom_segment(aes(x = bill_length_mm, xend = bill_length_mm,
                   y = model_predictions, yend = bill_depth_mm),
               linetype = "dashed") + 
  ggtitle("Illustration of linear regression residual errors",
          subtitle = "A sample of 25 observations")
```

הקווים המקווקים בין הנקודות לקו הרגרסיה מתארים את שגיאות הרגרסיה (השאריות). קו הרגרסיה מביא למינימום את סכום הריבועים של השגיאות הללו.

למודל הרגרסיה הלינארית יש שתי הנחות שקשורות לשגיאות:

-   הנחת הנורמליות של השגיאות: השגיאות מתפלגות נורמלית עם תוחלת 0

-   הנחת ההומוסקדסטיות של השגיאות: התפלגות השגיאות (פיזור השגיאות למעשה) לא תלויה במיקום התצפית (במקרה זה, לא תלויה בערך של `bill_length_mm`.

ניתן לחשב מודל רגרסיה גם במקרה שהנחות אלו לא מתקיימות אך המבחנים הסטטיסטיים שמתייחסים למודל ולמקדמי המודל אינם תקפים במידה והנחות הרגרסיה אינן מתקיימות.

## בדיקת הנחות הרגרסיה

כעת נבחן האם הנחות מודל הרגרסיה תקפות עבור מודל הרגרסיה שבנינו.

### התפלגות נורמלית

כפי שראינו ב[פרק @sec-hypothesis-tests], עומדים לרשותנו שני כלים על מנת לבחון התפלגות נורמלית: ויז'ואליזציה (כדוגמת q-q plot), ומבחן השערות (כדוגמת טיב התאמה או קולמוגורוב-סמירנוב). נשתמש ראשית בהמחשה ויזואלית ולאחר מכן במבחן סטטיסטי.

```{r qqplot of residuals}
tibble(resid = adelie_lm$residuals) %>% 
  ggplot(aes(sample = resid)) + 
  geom_qq() + 
  ggtitle("QQ plot of Adelie linear model residuals") + 
  geom_qq_line()
```

כפי שניתן לראות, יש חריגה מסוימת מההתפלגות הנורמלית, בעיקר בקצוות של ההתפלגות: מעבר לציון תקן 1 בציר ה-x ניתן לראות שהשאריות גבוהות מהצפוי (מעל הקו של y=x, בחלק הימני העליון של התרשים), ובאופן דומה (אך פחות בולט) בחלק התחתון-שמאלי של התרשים.

נשתמש במבחן קולמוגורוב-סמירנוב לבדיקת ההשערה שההתפלגות של השגיאות נורמלית. במסגרת קלט המבחן אנחנו משתמשים בפונקציה `scale` על מנת לוודא שסטיית התקן של השגיאות מנורמלת ל-1 (כפי שציינו מוקדם יותר בפרק זה, ממוצע השגיאות הוא 0 ממילא). לחילופין, ניתן היה לציין פרמטרים בפקודה אשר מגדירים לפונקציה לאיזה התפלגות נורמלית משווים הנתונים (כפי שהפעלנו אותה בפרק שדן במבחני השערות).

```{r ks test for normality}
ks.test(scale(adelie_lm$residuals)[, 1],
        "pnorm")
```

על פי פלט הפקודה, ה-p-value לדחיית השערת האפס הינו 0.5166, ולכן לא ניתן לדחות את השערת האפס שהשאריות מתפלגות נורמלית.

נשים לב לאזהרה שהפקודה מוציאה, שמתייחסת לכך שיש שתי תצפיות של שאריות שערכן זהה. מכיוון שהתפלגות נורמלית הינה התפלגות רציפה, ההסתברות לשתי תצפיות שוות הינה 0. מכיוון שהתצפיות שאנו משתמשים בהן הינן תצפיות מדידה, ישנן שתי תצפיות זהות עם ערך שארית זהה.

::: callout-tip
# תרגיל: בחינת נורמליות של השאריות

השתמשו בפונקציה `unique` על מנת להריץ את הפקודה מבלי לקבל את הודעת האזהרה של פקודת `ks.test`. האם השתנתה התובנה לגבי ההתפלגות הנורמלית של השאריות?

השתמשו במבחן טיב-התאמה במקום במבחן קולמוגורוב סמירנוב. האם השתנתה התובנה לגבי ההתפלגות הנורמלית של השאריות?
:::

### הומוסקדסטיות

על מנת לבחון הומוסקדסטיות, נרצה לראות האם יש הבדל בשונות של שגיאות המודל כפונקציה של המשתנה הבלתי תלוי. במקרים בהם משתמשים ברגרסיה רב משתנית (יותר ממשתנה בלתי תלוי אחד), ניתן לבצע בדיקה זו למול כל משתנה בלתי תלוי (או שלעיתים מבוצעת הבדיקה למול המשתנה התלוי, כקירוב).

```{r homoschedastity}
tibble(adelie_lm$model) %>% 
  mutate(resid = adelie_lm$residuals) %>% 
  ggplot(aes(x = bill_length_mm, y = resid)) + 
  geom_point() + 
  ggtitle("Model residuals against the bill length")
```

מהתרשים לא ניתן להבחין בהפרה משמעותית של הנחת ההומוסקדסטיות: הפיזור של התצפיות מסביב לציר ה-y=0 נראה כאחיד. ניתן גם לחלק את המשתנה למספר קבוצות, ולהמחיש את השונות בדרכים נוספות, לדוגמה באמצעות Boxplot. שיטה זו שימושית במיוחד כשמדובר במשתנים קטגוריאליים, אך ניתן להשתמש בה גם במקרה הנוכחי על ידי חלוקת המשתנה הבלתי תלוי לקטגוריות:

```{r homoscedasticity via boxplot}
homoscedasticity_test <- tibble(adelie_lm$model) %>% 
  mutate(resid = adelie_lm$residuals) %>% 
  mutate(bill_length_fct = cut(bill_length_mm, 
                               breaks = c(
                                 31, 37, 40, 42, 50
                               )))

homoscedasticity_test %>% 
  ggplot(aes(x = bill_length_fct, y = resid)) + 
  geom_boxplot() + 
  ggtitle("Model residual distribution against bill length groups (boxplot)")
```

בהסתכלות בתרשים ה-Boxplot נראה שיש הבדל מסוים בין הקבוצות. נשתמש במבחן הומוסקדסטיות לשונות על מנת לבחון את ההשערה להומוסקדסטיות (לעומת השערה אלטרנטיבית של הטרוסקדסטיות).

יש מספר מבחנים סטטיסטיים בהם ניתן להשתמש (לדוגמה במבחן F לשיוויון שונויות, שבו השתמשנו בפרק שעסק במבחני השערות). נדגים את המבחן הסטטיסטי באמצעות מבחן Bartlett's test ובאמצעות Levene's test.

```{r bartletts and levenes test}
bartlett.test(resid ~ bill_length_fct, 
              data = homoscedasticity_test)

car::leveneTest(resid ~ bill_length_fct, 
                data = homoscedasticity_test)
```

בשני המבחנים קיבלנו ערכי p-value\>0.05 ולכן לא ניתן לדחות את השערת האפס שהשונות בין הקבוצות שווה. ככל הנראה, הנחת ההומוסקדסטיות מתקיימת והמודל תקין. ככלל, ניתן להסתפק במבחן אחד מבין השניים.

## סידור פלט הפקודה

נציין שישנה דרך לחלץ את נתוני המקדמים של המודל כאובייקט tibble על ידי שימוש בפקודה `broom::tidy` באופן הבא (בדומה לשימוש בפקודה זו שראינו בפרק הקודם):

```{r demonstration of tidy}
broom::tidy(adelie_lm)
```

ישנן פקודות שונות שניתן להשתמש בהן על מנת להדפיס את תוצאות הרגרסיה בטבלה מסודרת (כזו שתתאים לפרסום), לדוגמה:

::: {style="direction: ltr;"}
```{r gt example linear regression}
#| warning: false
gtsummary::tbl_regression(adelie_lm)
```
:::

חבילות נוספות הינן `stargazer` לטבלאות, ו-`jtools` לויז'ואליזציות.\
לא נדון בחבילות אלו בפרק זה, אך הקורא המעוניין מוזמן להתעמק בחבילות וביכולות.

## רגרסיה מרובת משתנים

עד כה הדגמנו רגרסיה **חד-משתנית**, כלומר משתנה בלתי תלוי יחיד. כעת נרחיב את ההסתכלות למודל רגרסיה רב משתני: ישנם מספר משתנים בלתי-תלויים (ומשתנה תלוי אחד).

נשתמש במודל של רגרסיה לינארית לחיזוי ה-`bill_depth_mm`, באמצעות משתנים נוספים:

```{r multiple linear regression}
adelie_mult_lm <- 
  lm(formula = 
       bill_depth_mm ~ bill_length_mm + flipper_length_mm + body_mass_g + sex,
     data = adelie_penguins)

summary(adelie_mult_lm)
```

### אינטראקציה של משתנים

TBD

### המחשה של מקדמי הרגרסיה

TBD

::: end-page
